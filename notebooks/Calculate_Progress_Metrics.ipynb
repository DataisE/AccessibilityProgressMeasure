{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e38cf-3e4d-46af-b431-4f079d7a8152",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-11-12T19:16:21.8626437Z",
       "execution_start_time": "2024-11-12T19:16:21.4861918Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "cca8fbcb-a9a4-4b7a-b167-b5f662d9be23",
       "queued_time": "2024-11-12T19:16:20.0573021Z",
       "session_id": "7cf4afe6-0fa5-45a6-91c8-db5db7343901",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 7,
       "statement_ids": [
        7
       ]
      },
      "text/plain": [
       "StatementMeta(, 7cf4afe6-0fa5-45a6-91c8-db5db7343901, 7, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### Working Code ####\n",
    "import notebookutils, csv\n",
    "import os, json, time, pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "\n",
    "areas = [\"Employment\"]\n",
    "# , \"Information and Communication Technology\"]\n",
    "#        \"Communications other than ICT\",\n",
    "#         \"Design and Delivery of Programs and Services\", \"Procurement of goods, services and facilities\",\n",
    "#        \"Transportation\", \"Built Environment\"]\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"xxxxxxxxxx\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"xxxxxxxxxxxx\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2024-02-01\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26be01d0-504d-4e29-9cd6-75b45b597381",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-11-12T19:16:22.8578073Z",
       "execution_start_time": "2024-11-12T19:16:22.5409627Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "1c46af8d-5bb3-4819-a511-be93a5453ff9",
       "queued_time": "2024-11-12T19:16:20.1068078Z",
       "session_id": "7cf4afe6-0fa5-45a6-91c8-db5db7343901",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 8,
       "statement_ids": [
        8
       ]
      },
      "text/plain": [
       "StatementMeta(, 7cf4afe6-0fa5-45a6-91c8-db5db7343901, 8, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Working Code\n",
    "\n",
    "def json_extract(obj, key):\n",
    "    \"\"\"Recursively fetch values from nested JSON.\"\"\"\n",
    "    arr = []\n",
    "\n",
    "    def extract(obj, arr, key):\n",
    "        \"\"\"Recursively search for values of key in JSON tree.\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            for k, v in obj.items():\n",
    "                if isinstance(v, (dict, list)):\n",
    "                    extract(v, arr, key)\n",
    "                elif k == key:\n",
    "                    arr.append(v)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                extract(item, arr, key)\n",
    "        return arr\n",
    "\n",
    "    values = extract(obj, arr, key)\n",
    "    return values[0]\n",
    "\n",
    "def extract_metrics(sector, area_desc_pr, area_desc_pl, org_c, area_m, processed_file_path):\n",
    "    \n",
    "    res = []\n",
    "    for i in range(len(org_c)):\n",
    "\n",
    "        prompt = f\"Please compare the plan and the progress report for {area_m} of {org_c[i]} in {sector} as per the details in the plan : {area_desc_pl[i]} and the progress report: {area_desc_pr[i]}.\" \n",
    "        prompt = prompt + f\"Calculate an overall progress made in this area for accessibility improvements\"\n",
    "        prompt = prompt + \"Provide the output in the format : \"\n",
    "        prompt = prompt + \"{ \"\"\"\"Organization\"\" : \"\"\"\", \"\"Sector\"\" : \"\"\"\", \"\"Area\"\" : \"\"\"\", \"\"Analysis\"\" : \"\"\"\", \"\"Explanation\"\" : \"\"\"\", \"\"Progress\"\" : [ { \"\"Activity\"\" : \"\"\"\", \"\"Percent Progress\"\" : \"\"\"\"}], \"\"Total percent progress\"\" : \"\"\"\", \"\"Explanation of Percent Progress\"\" : \"\"\"\" }\"\n",
    "    \n",
    "        MESSAGES = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an accessibility compliance analyst.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "\n",
    "        time.sleep(60)\n",
    "        \n",
    "        client = AzureOpenAI()\n",
    "\n",
    "        # temp is set to 0 to get s consistent response from the model\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo-2024-04-09\",\n",
    "            temperature=0.0,\n",
    "            messages=MESSAGES\n",
    "        )\n",
    "        parsed_data = json.loads(response.json())\n",
    "        content = json_extract(parsed_data, \"content\")\n",
    "        \n",
    "        time.sleep(60)\n",
    "        create_or_append_json(content , processed_file_path)     \n",
    "    return(content)\n",
    "    \n",
    "\n",
    "\n",
    "def create_or_append_json(content, processed_file_path):\n",
    "\n",
    "    new_data = content\n",
    "\n",
    "    # Check if file exists and has content\n",
    "    if notebookutils.fs.exists(\"file:\" + processed_file_path):\n",
    "        # Read existing data\n",
    "        with open(processed_file_path, 'r') as file:\n",
    "            try:\n",
    "                data = json.load(file)\n",
    "                \n",
    "                # Handle different data structures\n",
    "                if isinstance(data, list):\n",
    "                    data.append(new_data)\n",
    "                elif isinstance(data, dict):\n",
    "                    if 'items' not in data:\n",
    "                        data['items'] = []\n",
    "                    data['items'].append(new_data)\n",
    "                else:\n",
    "                    # If neither list nor dict, wrap existing data and new data in a list\n",
    "                    data = [data, new_data]\n",
    "                    \n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Error: File contains invalid JSON. Creating new file...\")\n",
    "                data = {'items': [new_data]}\n",
    "    else:\n",
    "        # Create new file with initial data\n",
    "        data = {'items': [new_data]}\n",
    "    \n",
    "    # Write data back to file\n",
    "    with open(processed_file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c075c4ff-dddf-4674-b58a-a5336478355a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-11-12T19:27:55.9418184Z",
       "execution_start_time": "2024-11-12T19:25:42.5715401Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "df0fb90d-803e-4c13-8115-6b1aeeb3ec83",
       "queued_time": "2024-11-12T19:25:41.9439935Z",
       "session_id": "7cf4afe6-0fa5-45a6-91c8-db5db7343901",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 12,
       "statement_ids": [
        12
       ]
      },
      "text/plain": [
       "StatementMeta(, 7cf4afe6-0fa5-45a6-91c8-db5db7343901, 12, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"Organization\": \"Rogers in Telecom\",\n",
      "  \"Sector\": \"Telecommunications\",\n",
      "  \"Area\": \"Employment\",\n",
      "  \"Analysis\": \"Comparison of Planned Tasks and Progress\",\n",
      "  \"Explanation\": \"The progress report for 2023 outlines several initiatives that align with the planned tasks for 2024 and 2025, focusing on enhancing accessibility for employees with accessibility needs. The initiatives include recruitment process enhancements, development of an accessibility hub, efficiencies in medical accommodations, and a pilot project for ergonomic equipment.\",\n",
      "  \"Progress\": [\n",
      "    {\n",
      "      \"Activity\": \"Develop a plan to improve the accommodation process for employees\",\n",
      "      \"Percent Progress\": 50\n",
      "    },\n",
      "    {\n",
      "      \"Activity\": \"Create an assistive technology service catalogue\",\n",
      "      \"Percent Progress\": 0\n",
      "    },\n",
      "    {\n",
      "      \"Activity\": \"Create a support model for employees with accessibility needs\",\n",
      "      \"Percent Progress\": 0\n",
      "    },\n",
      "    {\n",
      "      \"Activity\": \"Include accessibility skills in job ads when hiring\",\n",
      "      \"Percent Progress\": 100\n",
      "    },\n",
      "    {\n",
      "      \"Activity\": \"Make improvements to the employee self-serve accommodation website\",\n",
      "      \"Percent Progress\": 50\n",
      "    }\n",
      "  ],\n",
      "  \"Total percent progress\": 40,\n",
      "  \"Explanation of Percent Progress\": \"The total percent progress of 40% is calculated based on the completion of the activities outlined in the plan for 2024 and 2025. Significant progress has been made in including accessibility skills in job ads and partially improving the accommodation process and the self-serve accommodation website. However, there has been no progress reported on creating an assistive technology service catalogue or a support model for employees with accessibility needs.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "metrics_file_path = \"/lakehouse/default/Files/Gold/metrics.json\"\n",
    "\n",
    "area_m = 'employment'\n",
    "area_desc_pr = []\n",
    "area_desc_pl = []\n",
    "org_c = []\n",
    "\n",
    "\n",
    "df_org = spark.sql(\"SELECT DISTINCT LOWER(org) as org FROM Accessibility_LH.dbo.Plan_Area_Extract\") # WHERE lower(sector) = 'telecom' and lower(org)='<orgname>'\")\n",
    "\n",
    "#display(df_org)\n",
    "# convert column to list\n",
    "orgs = df_org.select(\"org\").rdd.flatMap(lambda x: x).collect()\n",
    "#print(orgs)\n",
    "\n",
    "for org_filter in orgs:\n",
    "    for area_m in areas:\n",
    "        sql_plan = f\"SELECT * FROM Accessibility_LH.dbo.Plan_Area_Extract WHERE LOWER(org) = '{org_filter}' and LOWER(area) = LOWER('{area_m}')\"\n",
    "        sql_prog_rep = f\"SELECT * FROM Accessibility_LH.dbo.Progressreport_Area_Extract WHERE LOWER(org) = '{org_filter}' and LOWER(area) = LOWER('{area_m}')\"\n",
    "\n",
    "        df_plan = spark.sql(sql_plan)\n",
    "        df_prog_rep = spark.sql(sql_prog_rep)\n",
    "\n",
    "        #display(df_prog_rep)\n",
    "        #display(df_plan)\n",
    "\n",
    "        org_c.append(org_filter)\n",
    "        sector = df_plan.first()['sector']\n",
    "        area_desc_pr.append(df_prog_rep.first()['area_desc'])\n",
    "        area_desc_pl.append(df_plan.first()['area_desc'])\n",
    "        response = extract_metrics(sector, area_desc_pr, area_desc_pl, org_c, area_m, metrics_file_path)\n",
    "        print(response)\n",
    "#print(f\"{area_desc_pr} \\n\\n\")\n",
    "#print (f\"{area_desc_pl} \\n\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {
    "environmentId": "9404e2c8-317c-413e-8ed6-32fd3d16c20e",
    "workspaceId": "d63a3ade-af13-485a-8f48-9d233c335d35"
   },
   "lakehouse": {
    "default_lakehouse": "c4fcfdd3-d245-4b89-ae87-03b2041bed02",
    "default_lakehouse_name": "Accessibility_LH",
    "default_lakehouse_workspace_id": "d63a3ade-af13-485a-8f48-9d233c335d35"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
